{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improving Fashion MNIST classification using Convolutions",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marjansherafati/tensorflow-for-fashion-MNIST-classification/blob/master/Improving_Fashion_MNIST_classification_using_Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa",
        "colab_type": "text"
      },
      "source": [
        "#Improving fashion MNIST classification using Convolutions\n",
        "\n",
        "In the previous exercise (simple tensorflow neural net for fashion MNIST classification), I simply used a flatten layer, a relu layer and an output Softmax layer.\n",
        "\n",
        "I got an accuracy of 0.89 for the training set, and 0.87 for the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsRtq9OLorS",
        "colab_type": "code",
        "outputId": "a1d399ce-e54d-4a10-9f3a-4ce3caff1bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.5020 - acc: 0.8236\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3753 - acc: 0.8636\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.3355 - acc: 0.8786\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3130 - acc: 0.8853\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2950 - acc: 0.8923\n",
            "10000/10000 [==============================] - 0s 42us/sample - loss: 0.3547 - acc: 0.8743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In this exercise, I want to start using convolution and max pool layers and see if I can increase the accuracy of training by adding convolution\n",
        "\n",
        "in the cell below, I first reshape the input to make it suitable for the convolution layers (change it from a list of 60000 vectors, each 28*28, to a 60000 by 28 by 28 by 1 tensor!\n",
        "\n",
        "Next I apply two convolution and two max pooling layers.\n",
        "\n",
        "The rest of the code from here looks the same as the simple neural net I created previously"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tFgT1MMKi6",
        "colab_type": "code",
        "outputId": "1410679b-1e22-46ab-f650-69184a771ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 47s 782us/sample - loss: 0.4669 - acc: 0.8321\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 46s 770us/sample - loss: 0.3122 - acc: 0.8857\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 46s 763us/sample - loss: 0.2665 - acc: 0.9023\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 47s 781us/sample - loss: 0.2377 - acc: 0.9112\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 46s 764us/sample - loss: 0.2170 - acc: 0.9196\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.2644 - acc: 0.9032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI",
        "colab_type": "text"
      },
      "source": [
        "Accuracy has gone up to 91% on the validation data, which is a significant improvement in the right direction.\n",
        "\n",
        "I can run the model for more epochs, say 20. Doing that, we can see that the accuracy on the test set does not necessarily improve with more epochs, due to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "244OahpqORmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "708929b1-8a00-4fc4-8897-b41e8c9d7dbf"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 46s 760us/sample - loss: 0.4666 - acc: 0.8300\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 46s 764us/sample - loss: 0.3144 - acc: 0.8855\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 46s 773us/sample - loss: 0.2728 - acc: 0.9000\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 47s 781us/sample - loss: 0.2428 - acc: 0.9107\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 45s 749us/sample - loss: 0.2182 - acc: 0.9182\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 45s 748us/sample - loss: 0.1981 - acc: 0.9252\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 44s 741us/sample - loss: 0.1808 - acc: 0.9321\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 45s 742us/sample - loss: 0.1634 - acc: 0.9385\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 45s 746us/sample - loss: 0.1503 - acc: 0.9440\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 45s 745us/sample - loss: 0.1364 - acc: 0.9490\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 45s 756us/sample - loss: 0.1258 - acc: 0.9522\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 44s 741us/sample - loss: 0.1118 - acc: 0.9579\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 44s 739us/sample - loss: 0.1023 - acc: 0.9612\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 44s 740us/sample - loss: 0.0942 - acc: 0.9644\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 44s 728us/sample - loss: 0.0859 - acc: 0.9683\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 45s 751us/sample - loss: 0.0785 - acc: 0.9707\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 44s 739us/sample - loss: 0.0714 - acc: 0.9726\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 45s 749us/sample - loss: 0.0652 - acc: 0.9757\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 44s 734us/sample - loss: 0.0591 - acc: 0.9772\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 45s 746us/sample - loss: 0.0544 - acc: 0.9793\n",
            "10000/10000 [==============================] - 2s 249us/sample - loss: 0.4647 - acc: 0.9060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "Let's look at some outputs from the convolution layers, as I want to visuallly understand what features did convolution find for each class.\n",
        "\n",
        "The below code will show me the labels of the first 100 test examples. I can see that the labels at index 0, 23 and 28 are all the same (9, which was a shoe).\n",
        "\n",
        "I will then apply a convolution layer to these inputs and visualize the output. This way we can see the features that the convolution layer isolated for the class \"shoe\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6nX4QsOku6",
        "colab_type": "code",
        "outputId": "145d606c-c625-4a8f-c1bc-8c914827fff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FGsHhv6JvDx",
        "colab_type": "code",
        "outputId": "858e3ef4-dc5e-412c-91fd-8945c21ea413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=28\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD8CAYAAACxUoU3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux",
        "colab_type": "text"
      },
      "source": [
        "# Tuning the neural net parameters\n",
        "\n",
        "First, I will change the number of convolution filter from 32 to 16 and 64 to see the impact on the accuracy and runtime\n",
        "\n",
        " Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYRidBXpBPM",
        "colab_type": "code",
        "outputId": "96b076b7-309d-4c0e-ab31-16ced3bee5fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 11, 11, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 128)               51328     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 55,098\n",
            "Trainable params: 55,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 31s 515us/sample - loss: 0.1769 - acc: 0.9463\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 30s 506us/sample - loss: 0.0585 - acc: 0.9818\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 31s 512us/sample - loss: 0.0418 - acc: 0.9872\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 31s 519us/sample - loss: 0.0315 - acc: 0.9901\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 31s 520us/sample - loss: 0.0241 - acc: 0.9925\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.0315 - acc: 0.9901\n",
            "0.9901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WgQz03MQ2b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "49a6d3a2-918a-46d6-8426-33404689d5cd"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_27 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.1186 - acc: 0.9641\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0417 - acc: 0.9870\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0272 - acc: 0.9917\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0195 - acc: 0.9940\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0156 - acc: 0.9951\n",
            "10000/10000 [==============================] - 4s 388us/sample - loss: 0.0291 - acc: 0.9919\n",
            "0.9919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGnES0WFQvjW",
        "colab_type": "text"
      },
      "source": [
        "Next, I repeated the training with only one convolution layer and deleted the second convolution layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IQ8CLp2R61J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "26ceac6c-fc39-4009-bd1c-3148fa517c6c"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 36s 608us/sample - loss: 0.1483 - acc: 0.9554\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 36s 607us/sample - loss: 0.0519 - acc: 0.9840\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 36s 598us/sample - loss: 0.0331 - acc: 0.9897\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 36s 594us/sample - loss: 0.0218 - acc: 0.9931\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 36s 600us/sample - loss: 0.0145 - acc: 0.9952\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 0.0733 - acc: 0.9782\n",
            "0.9782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqBTx8ASRU0B",
        "colab_type": "text"
      },
      "source": [
        "Next, let's add more convolution layers and see how that affects the accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaQyX6TOSGF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "d46fcbaf-faf4-4748-94d7-262cda9b6702"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 47s 775us/sample - loss: 0.2621 - acc: 0.9195\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 46s 772us/sample - loss: 0.0958 - acc: 0.9698\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 46s 773us/sample - loss: 0.0721 - acc: 0.9774\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 46s 775us/sample - loss: 0.0593 - acc: 0.9816\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 47s 780us/sample - loss: 0.0498 - acc: 0.9843\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0615 - acc: 0.9820\n",
            "0.982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNR-t8H8Sixu",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's implement a callback function to stop training once we reach the desired accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqL9QuvwSpnK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "9d6f2f15-27cd-4bb2-9ff0-d4615330dcad"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks = [callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Epoch 1/5\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9544\n",
            "Reached 95% accuracy so cancelling training!\n",
            "60000/60000 [==============================] - 45s 749us/sample - loss: 0.1513 - acc: 0.9545\n",
            "10000/10000 [==============================] - 3s 286us/sample - loss: 0.0704 - acc: 0.9761\n",
            "0.9761\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}